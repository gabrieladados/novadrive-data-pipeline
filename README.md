# Do Data Warehouse ao Looker Studio: Projeto End-to-End de An√°lise para a concession√°ria Nova Drive

> Este projeto foi desenvolvido como parte do Bootcamp de Engenharia de Dados ministrado pelo professor Fernando Amaral. Ele consiste na constru√ß√£o de um Data Warehouse para uma concession√°ria fict√≠cia, com foco na cria√ß√£o de pipelines de dados eficientes e visuais impactantes.
>


## üìù Contexto

A **Nova Drive** √© uma concession√°ria que enfrenta dificuldades na √°rea de vendas para acessar e consumir dados de forma eficiente. Atualmente, a equipe precisa solicitar ao setor de TI a gera√ß√£o de relat√≥rios do CRM, que s√£o posteriormente exportados para o Excel e tratados manualmente. Esse processo √© demorado, propenso a erros e deixa os dados frequentemente desatualizados. Al√©m disso, h√° quest√µes de seguran√ßa, j√° que o uso de planilhas manipul√°veis pode comprometer a integridade das informa√ß√µes.

## üéØ Objetivo do Projeto

O objetivo deste projeto √© automatizar a **extra√ß√£o, carregamento e transforma√ß√£o (ELT)** dos dados, criando um pipeline que entregue um **dashboard** din√¢mico e atualizado em tempo real. Com isso, a equipe de vendas poder√° acessar informa√ß√µes seguras e precisas sobre os principais indicadores (KPIs) de forma √°gil, sem depender de processos manuais, garantindo maior efici√™ncia e confian√ßa na tomada de decis√µes.

## üõ†Ô∏è Arquitetura do Projeto

A arquitetura do projeto foi organizada em camadas, seguindo boas pr√°ticas de engenharia de dados.

1. **Camada de Ingest√£o**: Extra√ß√£o dos dados do PostgreSQL.
2. **Camada de Orquestra√ß√£o**: Automa√ß√£o dos processos de extra√ß√£o, transforma√ß√£o e carregamento usando Airflow.
3. **Camada de Armazenamento**: Armazenamento dos dados transformados no Snowflake.
4. **Camada Anal√≠tica**: Modelagem dos dados no dbt para gera√ß√£o de tabelas anal√≠ticas.
5. **Visualiza√ß√£o de Dados**: Dashboards criados no Looker Studio.

## üõ†Ô∏è Ferramentas Utilizadas

- **PostgreSQL**: Banco de dados de origem.
- **Airflow**: Orquestra√ß√£o e automa√ß√£o de pipelines.
- **Snowflake**: Data Warehouse na nuvem.
- **dbt**: Modelagem, testes e documenta√ß√£o de dados.
- **Looker Studio**: Visualiza√ß√£o de dados e constru√ß√£o de dashboards.
- **AWS EC2**: Hospedagem da m√°quina virtual.
   

## üöÄ Etapas do Projeto

### **Etapa 1: Conex√£o com o banco de dados e explora√ß√£o dos dados**

- Conex√£o com o banco de dados PostgreSQL usando o PgAdmin.
- Explora√ß√£o inicial das tabelas e suas rela√ß√µes.

### **Etapa 2: Configura√ß√£o da m√°quina virtual e do Airflow**

- Configura√ß√£o de uma inst√¢ncia EC2 no AWS com Ubuntu.
- Instala√ß√£o e configura√ß√£o do Docker e Airflow.

### **Etapa 3: Configura√ß√£o do Snowflake**

- Cria√ß√£o de uma conta no Snowflake.
- Configura√ß√£o de objetos no Snowflake (banco de dados, schemas, tabelas e warehouses).

### **Etapa 4: Constru√ß√£o do pipeline com Airflow**

- Desenvolvimento de DAGs para extra√ß√£o, transforma√ß√£o e carregamento (ETL) dos dados.
- Testes de carregamento de dados no Snowflake.

### **Etapa 5: Modelagem anal√≠tica com dbt**

- Cria√ß√£o de modelos anal√≠ticos no dbt.
- Valida√ß√£o e teste de integridade dos dados.
- Execu√ß√£o de jobs para gera√ß√£o de tabelas anal√≠ticas.

### **Etapa 6: Constru√ß√£o do Dashboard no Looker Studio**

- Integra√ß√£o do Snowflake com o Looker Studio.
- Cria√ß√£o de dashboards com os principais KPIs da empresa.


## Explora√ß√£o Incial dos Dados & Respondendo Quest√µes
1. Liste todos os ve√≠culos com tipo 'SUV Compacta' e valor inferior a 30.000,00.
    ```sql
    SELECT *
    FROM veiculos
    WHERE tipo = 'SUV Compacta' AND valor <30000.00
    ```
2. Exiba o nome dos clientes e o nome das concession√°rias onde realizaram suas compras.
       
   ```sql
   SELECT 
       cli.cliente,
       con.concessionaria
   FROM clientes AS cli
   JOIN concessionarias AS con
        ON cli.id_concessionarias = con.id_concessionarias
   ```

3. Conte quantos vendedores existem em cada concession√°ria.
       
   ```sql
    SELECT 
    	con.concessionaria AS nome_concessionaria,
    	COUNT(DISTINCT(ven.id_vendedores)) AS num_vendedores
    FROM concessionarias AS con
    JOIN vendedores AS ven
    ON con.id_concessionarias = ven.id_concessionarias
    GROUP BY nome_concessionaria
    ORDER BY num_vendedores DESC
   ```

4. Encontre os ve√≠culos mais caros vendidos em cada tipo de ve√≠culo.
   ```sql
    WITH base AS (
    
    SELECT *
    FROM vendas AS ven
    JOIN veiculos AS vei
    ON ven.id_veiculos = vei.id_veiculos
    ),
    
    mais_caros AS (
    SELECT 
    	tipo,
    	MAX(valor_pago) AS maior_valor
    FROM base
    GROUP BY tipo
    ORDER BY maior_valor DESC
    )
    
    SELECT *
    FROM mais_caros
        ORDER BY num_vendedores DESC
   ```
5. Liste o nome do cliente, o ve√≠culo comprado e o valor pago, para todas as vendas.
   ```sql
    SELECT 
    	cli.cliente,
    	vei.nome,
    	ven.valor_pago
    FROM vendas AS ven
    LEFT JOIN clientes AS cli
      ON ven.id_clientes = cli.id_clientes
    JOIN veiculos AS vei
      ON ven.id_veiculos = vei.id_veiculos
    ```
6. Identifique as concession√°rias que venderam mais de 5 ve√≠culos.
   ```sql
   SELECT 
    	con.concessionaria,
    	COUNT(ven.id_veiculos) AS num_veiculos
    FROM vendas AS ven
    JOIN concessionarias AS con
    ON ven.id_concessionarias = con.id_concessionarias
    GROUP BY con.concessionaria
    HAVING COUNT(ven.id_veiculos) > 5
   ```
    
7. Liste os tr√™s ve√≠culos mais caros dispon√≠veis.
   ```sql
    SELECT *
    FROM veiculos
    ORDER BY valor DESC
    LIMIT 3
   ```
       
8. Selecione todos os ve√≠culos adicionados no √∫ltimo m√™s.
   ```sql
    SELECT *
    FROM veiculos
    WHERE data_inclusao > CURRENT_TIMESTAMP - INTERVAL '1 month'
   ```
  
9. Liste todas as cidades e qualquer concession√°ria nelas, se houver.
   ```sql
    SELECT 
    	ciade,
    	concessionaria
    FROM cidades AS cid
    LEFT JOIN concessionarias AS con
      ON cid.id_cidades = con.id_cidadesd
    ```

10. Encontre clientes que compraram ve√≠culos 'SUV Premium H√≠brida' ou ve√≠culos com valor acima de 60.000,00.
 ```sql
    SELECT 
      	cli.cliente,
      	CASE 
          WHEN vei.tipo = 'SUV Premium H√≠brida' and ven.valor_pago > 60000 THEN 'SUV PH e >60k'
      		WHEN vei.tipo = 'SUV Premium H√≠brida' THEN 'SUV PH'
      		WHEN ven.valor_pago > 60000 THEN '>60K'
      	END as tipo_cliente
    FROM vendas AS ven
    JOIN clientes AS cli
       ON ven.id_clientes = cli.id_clientes
    JOIN veiculos AS vei
      ON ven.id_veiculos = vei.id_veiculos
    WHERE vei.tipo = 'SUV Premium H√≠brida' OR ven.valor_pago > 60000
   ```


## üöÄ Instala√ß√£o e Configura√ß√£o do Airflow com Docker

Para instalar e configurar o **Apache Airflow** utilizando o **Docker** no Ubuntu, siga os passos abaixo. Esses comandos ser√£o usados para configurar o Airflow em um ambiente desacoplado e garantir que ele esteja rodando corretamente.

</details>

<details>
<summary>
 Passo a passo da Instala√ß√£o
</summary>
  
  1. **Atualize a lista de pacotes do APT:**
     ```bash
      sudo apt-get update
      ```
  2. **Instale pacotes necess√°rios para adicionar um novo reposit√≥rio via HTTPS:**
      ```bash
      sudo apt-get install ca-certificates curl gnupg lsb-release
      ```
  
  3. **Crie o diret√≥rio para armazenar as chaves de reposit√≥rios:**
      ```bash
      sudo mkdir -m 0755 -p /etc/apt/keyrings
      ```
  
  4. **Adicione a chave GPG do reposit√≥rio do Docker:**
      ```bash
      curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
      ```
  
  5. **Adicione o reposit√≥rio do Docker √†s fontes do APT:**
      ```bash
      echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
      ```
  
  6. **Atualize a lista de pacotes ap√≥s adicionar o novo reposit√≥rio do Docker:**
      ```bash
      sudo apt-get update
      ```
  
  7. **Instale o Docker e seus componentes:**
      ```bash
      sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
      ```
  
  8. **Baixe o arquivo `docker-compose.yaml` do Airflow:**
      ```bash
      curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml'
      ```
  
  9. **Crie os diret√≥rios necess√°rios para DAGs, logs e plugins:**
      ```bash
      mkdir -p ./dags ./logs ./plugins
      ```
  
  10. **Crie o arquivo `.env` com o UID do usu√°rio para configura√ß√µes de permiss√µes do Docker:**
      ```bash
      echo -e "AIRFLOW_UID=$(id -u)" > .env
      ```
  
  11. **Inicie o Airflow com o comando:**
      ```bash
      sudo docker compose up airflow-init
      ```
  
  12. **Suba o Airflow em modo desacoplado:**
      ```bash
      sudo docker compose up -d
      ```
  
  13. **Aguarde at√© que o Airflow esteja pronto e saud√°vel.**
  
  14. **Acesse o Airflow atrav√©s do navegador utilizando o endere√ßo (substitua a URL abaixo pelo IP p√∫blico do seu servidor):**
      ```bash
      http://coloque_a_url_aqui:8080/
      ```
  
  15. **Edite o arquivo `docker-compose.yaml` e altere a configura√ß√£o `AIRFLOW__CORE__LOAD_EXAMPLES` para `false`:**
      ```bash
      nano /home/ubuntu/docker-compose.yaml
      # Mude para falso
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      ```
  
  16. **Reinicie o Airflow ap√≥s as altera√ß√µes:**
      ```bash
      sudo docker compose stop
      sudo docker compose up -d
      sudo docker ps
      ```
</details>



## üßë‚Äçüíª Modelagem dos Dados no dbt e Testes

A modelagem dos dados no dbt foi dividida em tr√™s etapas principais: **staging**, **intermediate** e **marts**.

1. **Staging:** Carrega os dados brutos para a √°rea de staging e renomeando campos necess√°rios sem modificar substancialmente os dados.

2. **Intermediate:** Realiza transforma√ß√µes iniciais, como limpeza de dados e cria√ß√£o de joins, criando as tabelas dimensionais e a fato. 

3. **Marts:** A camada Marts √© a etapa final da modelagem dimensional, onde os dados s√£o organizados para an√°lise de neg√≥cios, como vendas por per√≠odo, concession√°ria, ve√≠culo e vendedor. √â a camada consumida pelos usu√°rios finais, seja por consultas diretas no Snowflake ou por meio de ferramentas de BI, facilitando a extra√ß√£o de insights de forma r√°pida e eficiente.


### Testes no dbt

Durante a modelagem, foram aplicados testes para garantir a qualidade e a consist√™ncia dos dados, como a verifica√ß√£o de dados nulos e a unicidade das chaves prim√°rias. Al√©m de um teste em espec√≠fico para validar se os dados de acordo com as regras de neg√≥cio, s√£o elas:

1. O pre√ßo de venda do ve√≠culo deve ser igual ou inferior ao valor sugerido.
2. O desconto aplicado n√£o pode exceder 5% do valor do ve√≠culo.

#### Exemplos de comnados executados no dbt:
- **dbt run:** Executa os modelos e materializa-os no data warehouse.
- **dbt test ‚Äìselect source:*:** Realiza testes de qualidade nos dados de origem.
- **dbt test:** Valida os dados com base nas regras de neg√≥cio e condi√ß√µes do projeto.




## üìä Dashboard no Looker Studio

- **KPIs monitorados e An√°lises Realizadas**:
    - Quantidade de vendas.
    - Total de vendas.
    - Valor m√©dio de vendas
    - An√°lise temporal do total de vendas
    - Mapa da quantidade de vendas por estado.
    - Top 5 concession√°rias em total de vendas.
    - Top 5 ve√≠culos em total de vendas.
    - Top 5 vendedores em total de vendas.
    <br>  
      
  > üëâ **Acesse o dashboard no Looker aqui:** https://lookerstudio.google.com/reporting/084f3415-ca37-4d1c-ba6f-adfe0df6becd
![analise_vendas](https://github.com/user-attachments/assets/af7e1b92-4fb6-4842-9ecd-b755895f0ae3)



## üöÄ Conclus√£o
Ao final deste projeto, conseguimos construir uma solu√ß√£o de engenharia de dados robusta e eficiente para a Nova Drive. O pipeline desenvolvido, com o uso de ferramentas modernas como Airflow, Snowflake, dbt e Looker Studio, garante que a √°rea de vendas agora tenha acesso a dados atualizados em tempo real, sem depender de processos manuais demorados ou suscet√≠veis a erros.

O dashboard criado oferece uma visualiza√ß√£o clara e interativa dos principais KPIs, permitindo que os gestores da Nova Drive acompanhem o desempenho das vendas, identifiquem tend√™ncias e tomem decis√µes informadas de maneira √°gil e segura. Al√©m disso, a automa√ß√£o do processo elimina a necessidade de interven√ß√£o manual constante e garante a integridade e seguran√ßa dos dados, resolvendo os principais desafios que a equipe enfrentava.

Essa solu√ß√£o n√£o apenas otimiza o fluxo de trabalho, mas tamb√©m proporciona mais confian√ßa e agilidade na an√°lise de dados, dando √† Nova Drive uma vantagem estrat√©gica no mercado competitivo de vendas de ve√≠culos.


---


## Constribui√ß√µes

Muito obrigada por acompanhar meu projeto at√© aqui! üéâ

Contribui√ß√µes s√£o **muito bem-vindas**. Se voc√™ tem sugest√µes ou melhorias, fique √† vontade para abrir uma **issue** ou enviar um **pull request**.

Gostou do projeto? N√£o esque√ßa de dar uma ‚≠êÔ∏è! 


**Meus Contatos:**

üíª [LinkedIn](https://www.linkedin.com/in/gabrielasantanamorais/)  
üì© [E-mail](mailto:gabrielasmorais01@gmail.com)

**At√© a pr√≥xima!** üöÄ
